\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage{caption}
\captionsetup[figure]{font=scriptsize,labelfont=scriptsize}
% \usepackage{utopia}
\usepackage{animate}

\usetheme{Madrid}
\usecolortheme{default}

\title[Deep Reinforcement Learning]{Deep Reinforcement Learning}
\subtitle{Scientific Initiation}
\author[Resck]{Lucas Emanuel Resck Domingues}
\institute[FGV]
{
  Escola de Matemática Aplicada \\
  Fundação Getulio Vargas
}
\date[2020]
{March 2020}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}

\setbeamertemplate{caption}[numbered]

\begin{document}

    \frame{\titlepage}

    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents
    \end{frame}

    \section{Introduction}

        \begin{frame}
            \frametitle{Motivation}
        \end{frame}

        \begin{frame}
            \frametitle{Intuition}
        \end{frame}

    % \section{Imitation Learning}

    %     \begin{frame}
    %         \frametitle{Terminology}
    %         At time $t$:
    %         \begin{itemize}
    %             \item $s_t$: the state of the enviroment;
    %             \item $o_t$: the agent observation of the enviroment;
    %             \item $a_t$: the action taken;
    %             \item $\pi_{\theta}(a_t | o_t)$: probability distribution of taken $a_t$ given $o_t$.
    %         \end{itemize}
    %     \end{frame}

    %     \begin{frame}
    %         \frametitle{Imitation Learning goal}
    %         The goal of Imitation Learning is to find a good $\pi_{\theta}(a_t | o_t)$,
    %         in order to take good actions. But it doesn't work.

    %         \begin{figure}
    %             \includegraphics[scale=0.4]{figures/imitation_learning.png}
    %         \end{figure}

    %     \end{frame}

    \section{Reinforcement Learning}

        \begin{frame}
            \frametitle{Markov Decision Process}
            Markov Decision Process is a 4-tuple $(S, A, T, r)$:
            \begin{itemize}
                \item $S$: space of states $s$;
                \item $A$: space of actions $a$;
                \item $T$: transition operator, with probabilities $P(s_{t+1} | s_t, a_t)$;
            \item $r$: reward function $S \times A \rightarrow {\rm I\!R}, \ (s, a) \mapsto r(s, a)$.
            \end{itemize}
        \end{frame}

        \begin{frame}
            \frametitle{The goal of Reinforcement Learning}
            \begin{figure}
                \centering
                \caption{Reinforcement Learning world (\href{http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-4.pdf}{Deep RL Berkeley Course}).}
                \includegraphics[width=0.9\linewidth]{figures/goal_rl.png}
                \label{fig:1}
            \end{figure}
            $$p_{\theta}(\tau) = p_{\theta}(s_1, a_1, \cdots, s_T, a_T) = p(s_1) \prod_{t=1}^T \pi_{\theta}(a_t|s_t)p(s_t, a_t)$$
            $$\theta^* = \arg\max_\theta E_{\tau \sim p_\theta(\tau)}\left[\sum_t r(s_t, a_t)\right]$$
        \end{frame}

        \begin{frame}
            \frametitle{Too many algorithms}
            \begin{itemize}
                \item Policy iteration;
                \item Value iteration;
                \item Q-learning;
                \item Deep Q-learning.
            \end{itemize}
        \end{frame}

        \begin{frame}
            \frametitle{More definitions}
            \begin{block}{Q-Function}
                $$Q^\pi(s_t, a_t) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t'}, a_{t'})|s_t, a_t]$$

                The total expected reward for taking an action $a_t$ when the state is $s_t$.
            \end{block}
            \begin{block}{Value Function}
                $$V^\pi(s_t) = E_{a_t \sim \pi_\theta(a_t|s_t)}[Q^\pi(s_t, a_t)]$$

                The total expected reward when the state is $s_t$.
            \end{block}
        \end{frame}

        \begin{frame}
            \frametitle{Policy iteration}
            \begin{figure}
                \centering
                \includegraphics[width=0.6\linewidth]{figures/policy_iteration.png}
                \label{fig2:policy}
                \caption{Policy iteration (Sutton and Andrew, Reinforcement Learning: An Introduction, 2nd edition).}
            \end{figure}
        \end{frame}

        \begin{frame}
            \frametitle{Policy iteration example: FrozenLake8x8}
            \begin{figure}
                \centering
                \includegraphics[width=0.2\linewidth]{figures/frozenlake8x8}
                \label{fig4:frozenlake8x8}
                \caption{FronzenLake8x8 environment.}
            \end{figure}
        \end{frame}

        \begin{frame}
            \frametitle{Value iteration}
            \begin{figure}
                \centering
                \includegraphics[width=0.7\linewidth]{figures/value_iteration.png}
                \label{fig5:value}
                \caption{Value iteration (Sutton and Andrew, Reinforcement Learning: An Introduction, 2nd edition).}
            \end{figure}
        \end{frame}

        \begin{frame}
            \frametitle{Value iteration example: FrozenLake8x8}
            \begin{figure}
                \centering
                \includegraphics[width=0.2\linewidth]{figures/frozenlake8x8}
                \label{fig6:frozenlake8x8}
                \caption{FronzenLake8x8 environment.}
            \end{figure}
        \end{frame}

        \begin{frame}
            \frametitle{Q-Learning}
            \begin{figure}
                \centering
                \includegraphics[width=0.8\linewidth]{figures/q_learning.png}
                \label{fig6:q}
                \caption{Q-Learning (Sutton and Andrew, Reinforcement Learning: An Introduction, 2nd edition).}
            \end{figure}
        \end{frame}

        \begin{frame}
            \frametitle{Q-Learning example}
            \begin{figure}
                \centering
                \fbox{\includegraphics[width=0.7\linewidth]{figures/mountaincar.png}}
                \label{fig7:mountain}
                \caption{MountainCar enviroment.}
            \end{figure}
        \end{frame}

        \begin{frame}
            \frametitle{Approximate Q-Learning}
            We assume the existence of a feature function $f: (s, a) \mapsto f_1(s, a), \cdots, f_n(s, a)$, with $f_i(s, a)$ a feature value.
            \begin{block}{Approximate Q-Function}
                $$Q(s, a) = \sum_{i = 1}^n f_i(s, a) w_i$$
            \end{block}
            \begin{block}{Approximate Q-learning iteration}
                $$w_i \leftarrow w_i + \alpha \cdot difference \cdot f_i(s, a)$$
                $$difference = (r + \gamma \max_{a'} Q(s', a')) - Q(s, a)$$
            \end{block}
        \end{frame}

        \begin{frame}
            \frametitle{Approximate Q-Learning example}
            \begin{figure}
                \centering  
                \includegraphics[width=0.8\linewidth]{figures/pacman.png}              
                \caption{PacMan environment (\href{http://ai.berkeley.edu/reinforcement.html}{Berkeley Intro to AI}).}
                \label{fig8:pacman}
            \end{figure}
        \end{frame}

    % Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. - Alan Turing
    
\end{document}